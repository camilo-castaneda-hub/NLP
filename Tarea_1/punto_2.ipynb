{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "## Punto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de estrategias de motores de búsqueda\n",
    "A continuación, implementará un motor de búsqueda con cuatro estrategias diferentes.\n",
    "1. Búsqueda binaria usando índice invertido (BSII)\n",
    "3. Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "Debe hacer su propia implementación usando numpy y pandas.\n",
    "\n",
    "Conjunto de datos: hay tres archivos que componen el conjunto de datos: \n",
    "- “Docs raws texts\" contiene 331 documentos en formato NAF (XML; debe usar el título y el \n",
    "contenido para modelar cada documento). \n",
    "- \"Queries raw texts\" contiene 35 consultas. \n",
    "- \"relevance-judgments.tsv\" contiene para cada consulta los documentos considerados relevantes \n",
    "para cada una de las consultas. Estos documentos relevantes fueron construidos manualmente por \n",
    "jueces humanos y sirven como “ground-truth” y evaluación.\n",
    "\n",
    "Pasos de preprocesamiento: para los siguientes puntos, debe preprocesar documentos y consultas \n",
    "mediante tokenización a nivel de palabra, eliminación de palabras vacías, normalización y stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 326.8 kB/s eta 0:00:05\n",
      "     --- ------------------------------------ 0.1/1.5 MB 944.1 kB/s eta 0:00:02\n",
      "     --------------------------------- ------ 1.3/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/de/cd/d80c9e284ae6c1b2172dacf0651d25b78ee1f7efbc12d74ea7b87c766263/regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\57305\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.3/268.3 kB 16.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB ? eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            # Create the folder within the target directory\n",
    "            os.mkdir(target_folder)\n",
    "\n",
    "        \n",
    "            # Extract all files to the target folder\n",
    "            zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths a directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios que contienen los archivos necesarios, cambiar acá si es necesario\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "queries_directory = \"queries-raw-texts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Índice invertido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# NLTK setup\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función de preprocesamiento, se usará para todos los inputs al modelo (queries y documentos)\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Realiza el preprocesamiento de un texto.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a procesar.\n",
    "\n",
    "    Returns:\n",
    "        List: Una lista con las palabras del texto, tokenizadas y sin palabras vacías.\n",
    "    \"\"\"\n",
    "    text.strip().lower() # normalización del texto, todo en minúscula y se quitan espacios innecesarios.\n",
    "    tokens = tokenizer.tokenize(text) #tokenización por espacio\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] # eliminación de palabras vacias y stemming\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " '26',\n",
       " 'juli',\n",
       " '2023',\n",
       " 'coup',\n",
       " 'état',\n",
       " 'occur',\n",
       " 'republ',\n",
       " 'niger',\n",
       " 'countri',\n",
       " 'presidenti',\n",
       " 'guard',\n",
       " 'remov',\n",
       " 'detain',\n",
       " 'presid',\n",
       " 'moham',\n",
       " 'bazoum']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\" On 26 July 2023, a coup d'état ocCuRreD in the Republic of the Niger, in which the country's presidential guard removed...; and detained President Mohamed Bazoum.    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del diccionario para el índice invertido con base en el corpus de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts raw text from .naf files\n",
    "def extract_raw_text(xml_path: str, title: bool = False) -> str:\n",
    "    \"\"\"Extrae el texto sin procesar de un archivo .naf.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): La ruta al archivo .naf.\n",
    "        title (bool): Si es True, el título del documento también se agrega al texto extraído.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto sin procesar del archivo .naf.\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find(\".//nafHeader/fileDesc\").get(\"title\") + \", \" + root.find('raw').text #Añade título cuando se especifíca\n",
    "    else:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find('raw').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the inverted index (term -> list of documents)\n",
    "inverted_index = {}\n",
    "\n",
    "# Dictionary to store term frequencies per document (term -> {document: frequency})\n",
    "term_freq_per_document = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        # Extract content from the XML\n",
    "        content = extract_raw_text(xml_path, title=True)\n",
    "        \n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index and update term frequencies per document\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                if filename not in inverted_index[term]:\n",
    "                    inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "            \n",
    "            if term in term_freq_per_document:\n",
    "                if filename in term_freq_per_document[term]:\n",
    "                    term_freq_per_document[term][filename] += 1\n",
    "                else:\n",
    "                    term_freq_per_document[term][filename] = 1\n",
    "            else:\n",
    "                term_freq_per_document[term] = {filename: 1}\n",
    "\n",
    "print(\"Inverted index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of documents where the term is found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wes2015.d137.naf', 'wes2015.d280.naf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"list of documents where the term is found\")\n",
    "test_term = preprocess_text(\"confused\")[0]\n",
    "inverted_index[test_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency value of term:\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"frequency value of term:\")\n",
    "for term in inverted_index[test_term]:\n",
    "    print(term_freq_per_document[test_term][term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(inverted_index: dict, query: str) -> list[str]:\n",
    "    \"\"\"Realiza una consulta booleana sobre un índice invertido.\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict): El índice invertido.\n",
    "        query (str): La consulta booleana.\n",
    "\n",
    "    Returns:\n",
    "        List: Una lista con los documentos que satisfacen la consulta.\n",
    "    \"\"\"\n",
    "    # Tokenize the query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Initialize result set with documents containing the first term\n",
    "    result_set = set(inverted_index.get(query_tokens[0], []))\n",
    "\n",
    "    # Iterate over query tokens and perform AND and NOT operations\n",
    "    operator = None\n",
    "    for token in query_tokens[1:]:\n",
    "        if token == 'and':\n",
    "            operator = 'AND'\n",
    "        elif token == 'not':\n",
    "            operator = 'NOT'\n",
    "        else:\n",
    "            if operator == 'AND':\n",
    "                result_set &= set(inverted_index.get(token, []))\n",
    "            elif operator == 'NOT':\n",
    "                result_set -= set(inverted_index.get(token, []))\n",
    "            else:\n",
    "                result_set |= set(inverted_index.get(token, []))\n",
    "            operator = None\n",
    "    # If the operator is 'AND', the result set is updated using set intersection (&).\n",
    "    # If the operator is 'NOT', the result set is updated using set difference (-).\n",
    "    # If no operator is set, the result set is updated using set union (|).\n",
    "    return list(result_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d280.naf', 'wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d105.naf', 'wes2015.d241.naf', 'wes2015.d193.naf', 'wes2015.d019.naf', 'wes2015.d102.naf', 'wes2015.d212.naf', 'wes2015.d316.naf', 'wes2015.d190.naf', 'wes2015.d247.naf', 'wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused AND Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d280.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused NOT Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d174.naf', 'wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused OR Anne AND Language NOT william\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_results = {}\n",
    "\n",
    "for filename in os.listdir(queries_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        query_path = os.path.join(queries_directory, filename)\n",
    "        \n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(query_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract query content from the XML\n",
    "        query_id = root.find('nafHeader/public').get('publicId')\n",
    "        query_content = root.find('raw').text\n",
    "        \n",
    "        # Preprocess the query content\n",
    "        preprocessed_query = preprocess_text(query_content)\n",
    "        queries_results[query_id] = preprocessed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform AND queries using the inverted index\n",
    "results = {}\n",
    "for query_id, query_terms in queries_results.items():\n",
    "    result_docs = set(inverted_index.get(query_terms[0], []))\n",
    "    for term in query_terms[1:]:\n",
    "        result_docs &= set(inverted_index.get(term, []))\n",
    "    results[query_id] = result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results written to BSII-AND-queries_results.tsv\n"
     ]
    }
   ],
   "source": [
    "# Write the results to a file\n",
    "output_filename = 'BSII-AND-queries_results.tsv'\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    for query_id, docs in results.items():\n",
    "        doc_numbers = [doc.split('.')[1] for doc in docs]\n",
    "        doc_numbers_str = ','.join(doc_numbers)\n",
    "        output_file.write(f\"{query_id}\\t{doc_numbers_str}\\n\")\n",
    "\n",
    "print(\"Query results written to\", output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
