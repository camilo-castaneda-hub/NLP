{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación ranqueada y vectorización de documentos (RRDV) usando GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/ad/97/b8253236dfedb9094f4273393a3fd03997da81f27f15822e56128da894ae/gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\57305\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.25.2)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Obtaining dependency information for scipy>=1.7.0 from https://files.pythonhosted.org/packages/06/15/e73734f9170b66c6a84a0bd7e03586e87e77404e2eb8e34749fc49fa43f7/scipy-1.11.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.1 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 41.0/59.1 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.1/59.1 kB 626.5 kB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.1/24.0 MB 36.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.8/24.0 MB 35.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.4/24.0 MB 35.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.1/24.0 MB 35.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.8/24.0 MB 35.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/24.0 MB 35.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.1/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 12.7/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.4/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.7/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.3/24.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.6/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "   ---------------------------------------- 0.0/44.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.9/44.0 MB 62.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 3.6/44.0 MB 46.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 5.3/44.0 MB 42.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 6.9/44.0 MB 40.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.6/44.0 MB 39.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.2/44.0 MB 38.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 11.8/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 13.5/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 15.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 16.8/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 18.5/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.1/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.8/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 23.4/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 25.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 26.5/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.0/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.6/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.0/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.6/44.0 MB 34.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.2/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.9/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.0/44.0 MB 21.8 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.2 scipy-1.11.2 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from metrics import precision_at_k, recall_at_k, ndcg_at_k, mean_average_precision\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            # Create the folder within the target directory\n",
    "            os.mkdir(target_folder)\n",
    "\n",
    "        \n",
    "            # Extract all files to the target folder\n",
    "            zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts raw text from .naf files\n",
    "def extract_raw_text(xml_path: str, title: bool = False) -> str:\n",
    "    \"\"\"Extrae el texto sin procesar de un archivo .naf.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): La ruta al archivo .naf.\n",
    "        title (bool): Si es True, el título del documento también se agrega al texto extraído.\n",
    "\n",
    "    Returns:\n",
    "        Str: El texto sin procesar del archivo .naf.\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find(\".//nafHeader/fileDesc\").get(\"title\") + \", \" + root.find('raw').text #Añade título cuando se especifíca\n",
    "    else:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find('raw').text\n",
    "\n",
    "# filtra los resultados de las listas para solo incluir los nombres de los documentos\n",
    "def filter_result_list(input_list):\n",
    "    return[full_result[:full_result.find(\":\")] for full_result in input_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevance judgments from the file\n",
    "relevance_judgments_file = open(\"relevance-judgments.tsv\", \"r\")\n",
    "relevance_judgments = {}\n",
    "\n",
    "for line in relevance_judgments_file:\n",
    "    query, judgments = line.strip().split('\\t')\n",
    "    relevance_judgments[query] = judgments.split(',')\n",
    "\n",
    "relevance_judgments_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\57305\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = gensim.utils.simple_preprocess\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = gensim.parsing.preprocessing.PorterStemmer()\n",
    "\n",
    "# Create a stopwords list\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesa un texto para eliminar palabras vacías, aplicar stemming y convertir a minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List: Una lista con las palabras del texto preprocesado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir a minúsculas y eliminar espacios innecesarios\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # Tokenizar por espacio\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    # Eliminar palabras vacías\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Aplicar stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios que contienen los archivos necesarios, cambiar acá si es necesario\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "relevance_judgments_directory = \"relevance-judgments.tsv\"\n",
    "\n",
    "queries_directory = \"queries-raw-texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the inverted index (term -> list of documents)\n",
    "inverted_index = {}\n",
    "\n",
    "# Dictionary to store term frequencies per document (term -> {document: frequency})\n",
    "term_freq_per_document = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        content = extract_raw_text(xml_path, title=True)\n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index and update term frequencies per document\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                if filename not in inverted_index[term]:\n",
    "                    inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "            \n",
    "            if term in term_freq_per_document:\n",
    "                if filename in term_freq_per_document[term]:\n",
    "                    term_freq_per_document[term][filename] += 1\n",
    "                else:\n",
    "                    term_freq_per_document[term][filename] = 1\n",
    "            else:\n",
    "                term_freq_per_document[term] = {filename: 1}\n",
    "\n",
    "print(\"Inverted index created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector for query: William Beaumont is Confused by human physiology\n",
      "[0.72338035 1.23482128 0.69954442 ... 0.         0.         0.        ]\n",
      "elements: 12647\n",
      "non-zero elements:3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def preprocess_input(query_string):\n",
    "    query_terms = gensim.utils.simple_preprocess(query_string)\n",
    "    return {term: 1 for term in query_terms}\n",
    "\n",
    "# Function to calculate TF-IDF vector for a query\n",
    "def calculate_tfidf_vector(input, inverted_index, term_freq_per_document):\n",
    "    total_documents = len(term_freq_per_document)\n",
    "    query = preprocess_input(input)\n",
    "    term_indices = {}  # Map term indices to integer indices in the tfidf_vector\n",
    "\n",
    "    for term_index, term in enumerate(inverted_index):\n",
    "        term_indices[term] = term_index\n",
    "\n",
    "    tfidf_vector = np.zeros(len(inverted_index))  # Initialize a vector of zeros\n",
    "\n",
    "    for term, term_index in inverted_index.items():\n",
    "        tf = query.get(term, 0)  # Term frequency in the query\n",
    "        df = len(term_index)\n",
    "\n",
    "        tfidf = (np.log10(1 + tf)) * np.log10(total_documents / df)\n",
    "        index = term_indices[term]\n",
    "        tfidf_vector[index] = tfidf\n",
    "\n",
    "    return tfidf_vector\n",
    "\n",
    "# Example usage\n",
    "query_string = \"William Beaumont is Confused by human physiology\"\n",
    "tfidf_vector = calculate_tfidf_vector(query_string, inverted_index, term_freq_per_document)\n",
    "print(\"TF-IDF Vector for query:\", query_string)\n",
    "print(tfidf_vector)\n",
    "print(f\"elements: {len(tfidf_vector)}\")\n",
    "print(f\"non-zero elements:{sum(tfidf_vector>0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Se preprocesa la consulta para eliminar stop words y se crea un diccionario que mapea cada término a su frecuencia en la consulta.\n",
    "\n",
    "b. Se crea un vector de longitud igual al número de términos en el índice invertido. El vector se inicializa a 0.\n",
    "\n",
    "c. Para cada término en el índice invertido, se calcula su TF-IDF. El TF-IDF se calcula como el producto de la frecuencia del término en la consulta y el logaritmo del número de documentos que contienen el término dividido por el número total de documentos.\n",
    "\n",
    "d. El valor TF-IDF del término se almacena en la posición del término en el vector.\n",
    "\n",
    "La estrategia utilizada es eficiente porque aprovecha la estructura del índice invertido. El índice invertido almacena una lista de documentos para cada término. Esto significa que podemos calcular la frecuencia de un término en una consulta en tiempo constante.\n",
    "\n",
    "Además, el cálculo del TF-IDF también es eficiente. El TF-IDF se calcula como el producto de la frecuencia de un término en una consulta y el logaritmo del número de documentos que contienen el término. El logaritmo es una operación relativamente costosa, pero se puede realizar en tiempo constante utilizando una tabla de logaritmos precalculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    # Asegúrate de que los vectores tengan la misma longitud\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise ValueError(\"Los vectores deben tener la misma longitud\")\n",
    "    \n",
    "    # Calcula la similitud del coseno\n",
    "    similarity = matutils.cossim(vector1, vector2)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rank_documents(query_text):\n",
    "    query_bow = inverted_index.doc2bow(query_text)\n",
    "    query_tfidf = calculate_tfidf_vector[query_bow]\n",
    "    sims = index[query_tfidf]\n",
    "    \n",
    "    ranked_documents = sorted(enumerate(sims), key=lambda item: -item[1])  # Ordenar por similitud en orden descendente\n",
    "    ranked_documents = [(f'd{idx + 1:02}', score) for idx, score in ranked_documents if score > 0]  # Filtrar similitud > 0\n",
    "    \n",
    "    return ranked_documents\n",
    "\n",
    "# Paso 4: Iterar sobre las consultas y escribir los resultados\n",
    "output_filename = \"GENSIM-consultas_resultados.tsv\"\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    print(\"Los resultados en {} ya existen.\".format(output_filename))\n",
    "else:\n",
    "    # Abre el archivo de resultados en modo escritura\n",
    "    with open(output_filename, \"w\") as results_file:\n",
    "        # Itera sobre las consultas\n",
    "        for i, query_text in enumerate(queries):\n",
    "            ranked_documents = retrieve_and_rank_documents(query_text)\n",
    "            \n",
    "            # Escribir resultados en el archivo\n",
    "            result_line = \"q{:02d} \".format(i + 1) + \", \".join([\"{}:{:.6f}\".format(doc, score) for doc, score in ranked_documents])\n",
    "            results_file.write(result_line + \"\\n\")\n",
    "            print(\"Consulta finalizada: q{:02d}\".format(i + 1))\n",
    "\n",
    "print(\"Los resultados se han escrito en \" + output_filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
