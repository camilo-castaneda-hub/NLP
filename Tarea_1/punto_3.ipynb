{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación ranqueada y vectorización de documentos (RRDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelve a crear el indice invertido junto a sus frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/9e/71/756a1be6bee0209d8c0d8c5e3b9fc72c00373f384a4017095ec404aec3ad/pandas-2.0.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\57305\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "     ---------------------------------------- 0.0/502.3 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 30.7/502.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 327.7/502.3 kB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 502.3/502.3 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 341.8/341.8 kB 22.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\57305\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\57305\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.5/10.6 MB 48.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.2/10.6 MB 40.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.8/10.6 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.5/10.6 MB 37.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.6 MB 36.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.6 MB 36.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            # Create the folder within the target directory\n",
    "            os.mkdir(target_folder)\n",
    "        \n",
    "            # Extract all files to the target folder\n",
    "            zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths a directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios que contienen los archivos necesarios, cambiar acá si es necesario\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "relevance_judgments_directory = \"relevance-judgments.tsv\"\n",
    "\n",
    "queries_directory = \"queries-raw-texts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\57305\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "# NLTK setup\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función de preprocesamiento, se usará para todos los inputs al modelo (queries y documentos)\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesa un texto para eliminar palabras vacías, aplicar stemming y convertir a minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List: Una lista con las palabras del texto preprocesado.\n",
    "    \"\"\"\n",
    "    text.strip().lower() # normalización del texto, todo en minúscula y se quitan espacios innecesarios.\n",
    "    tokens = tokenizer.tokenize(text) #tokenización por espacio\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] # eliminación de palabras vacias y stemming\n",
    "    return tokens # retorna lista con el texto tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts raw text from .naf files\n",
    "def extract_raw_text(xml_path: str, title: bool = False) -> str:\n",
    "    \"\"\"Extrae el texto sin procesar de un archivo .naf.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): La ruta al archivo .naf.\n",
    "        title (bool): Si es True, el título del documento también se agrega al texto extraído.\n",
    "\n",
    "    Returns:\n",
    "        Str: El texto sin procesar del archivo .naf.\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find(\".//nafHeader/fileDesc\").get(\"title\") + \", \" + root.find('raw').text #Añade título cuando se especifíca\n",
    "    else:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Extract content from the XML\n",
    "        return root.find('raw').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the inverted index (term -> list of documents)\n",
    "inverted_index = {}\n",
    "\n",
    "# Dictionary to store term frequencies per document (term -> {document: frequency})\n",
    "term_freq_per_document = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        content = extract_raw_text(xml_path, title=True)\n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index and update term frequencies per document\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                if filename not in inverted_index[term]:\n",
    "                    inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "            \n",
    "            if term in term_freq_per_document:\n",
    "                if filename in term_freq_per_document[term]:\n",
    "                    term_freq_per_document[term][filename] += 1\n",
    "                else:\n",
    "                    term_freq_per_document[term][filename] = 1\n",
    "            else:\n",
    "                term_freq_per_document[term] = {filename: 1}\n",
    "\n",
    "print(\"Inverted index created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF\\text{-}IDF_{t,d} = \\log(1 + \\text{TF}_{t,d}) \\times \\log \\left( \\frac{N}{\\text{DF}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector for query: William Beaumont is Confused by human physiology\n",
      "[0.24710288 0.75854381 0.22326695 ... 0.         0.         0.        ]\n",
      "elements: 13631\n",
      "non-zero elements: 5\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(query_string):\n",
    "    query_terms = preprocess_text(query_string)\n",
    "    return {term: 1 for term in query_terms}\n",
    "\n",
    "# Function to calculate TF-IDF vector for a query\n",
    "def calculate_tfidf_vector(input, inverted_index, term_freq_per_document, xml_files_directory=xml_files_directory):\n",
    "    total_documents = len(os.listdir(xml_files_directory))\n",
    "    query = preprocess_input(input)\n",
    "    term_indices = {}  # Map term indices to integer indices in the tfidf_vector\n",
    "    \n",
    "    for term_index, term in enumerate(inverted_index):\n",
    "        term_indices[term] = term_index\n",
    "    \n",
    "    tfidf_vector = np.zeros(len(inverted_index))  # Initialize a vector of zeros\n",
    "    \n",
    "    for term, term_index in inverted_index.items():\n",
    "        tf = query.get(term, 0)  # Term frequency in the query\n",
    "        df = len(term_index)\n",
    "        \n",
    "        tfidf = (np.log10(1 + tf)) * np.log10(total_documents / df)\n",
    "        index = term_indices[term]\n",
    "        tfidf_vector[index] = tfidf\n",
    "    \n",
    "    return tfidf_vector\n",
    "\n",
    "# Example usage\n",
    "query_string = \"William Beaumont is Confused by human physiology\"\n",
    "tfidf_vector = calculate_tfidf_vector(query_string, inverted_index, term_freq_per_document)\n",
    "print(\"TF-IDF Vector for query:\", query_string)\n",
    "print(tfidf_vector)\n",
    "print(\"elements: {}\".format(len(tfidf_vector)))\n",
    "print(\"non-zero elements: {}\".format(sum(tfidf_vector>0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    if norm_vector1 == 0 or norm_vector2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished query wes2015.q01.naf\n",
      "finished query wes2015.q02.naf\n",
      "finished query wes2015.q03.naf\n",
      "finished query wes2015.q04.naf\n",
      "finished query wes2015.q06.naf\n",
      "finished query wes2015.q07.naf\n",
      "finished query wes2015.q08.naf\n",
      "finished query wes2015.q09.naf\n",
      "finished query wes2015.q10.naf\n",
      "finished query wes2015.q12.naf\n",
      "finished query wes2015.q13.naf\n",
      "finished query wes2015.q14.naf\n",
      "finished query wes2015.q16.naf\n",
      "finished query wes2015.q17.naf\n",
      "finished query wes2015.q18.naf\n",
      "finished query wes2015.q19.naf\n",
      "finished query wes2015.q22.naf\n",
      "finished query wes2015.q23.naf\n",
      "finished query wes2015.q24.naf\n",
      "finished query wes2015.q25.naf\n",
      "finished query wes2015.q26.naf\n",
      "finished query wes2015.q27.naf\n",
      "finished query wes2015.q28.naf\n",
      "finished query wes2015.q29.naf\n",
      "finished query wes2015.q32.naf\n",
      "finished query wes2015.q34.naf\n",
      "finished query wes2015.q36.naf\n",
      "finished query wes2015.q37.naf\n",
      "finished query wes2015.q38.naf\n",
      "finished query wes2015.q40.naf\n",
      "finished query wes2015.q41.naf\n",
      "finished query wes2015.q42.naf\n",
      "finished query wes2015.q44.naf\n",
      "finished query wes2015.q45.naf\n",
      "finished query wes2015.q46.naf\n",
      "Results written to RRDV-consultas_resultados.tsv\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve and rank documents based on cosine similarity scores\n",
    "def retrieve_and_rank_documents(query_string, inverted_index, term_freq_per_document, xml_files_directory=xml_files_directory):\n",
    "    similarity_scores = {}  # Dictionary to store cosine similarity scores\n",
    "    query_vector = calculate_tfidf_vector(query_string, inverted_index, term_freq_per_document)\n",
    "    \n",
    "    for document in os.listdir(xml_files_directory):\n",
    "        if document.endswith('.naf'):\n",
    "            document_path = os.path.join(xml_files_directory, document)\n",
    "            document_text = extract_raw_text(document_path, title=True)\n",
    "            document_vector = calculate_tfidf_vector(document_text, inverted_index, term_freq_per_document, xml_files_directory=xml_files_directory)\n",
    "            similarity = calculate_cosine_similarity(query_vector, document_vector)\n",
    "            \n",
    "            if similarity > 0:\n",
    "                similarity_scores[document[:-4]] = similarity  # Remove the \".naf\" extension\n",
    "    \n",
    "    ranked_documents = sorted(similarity_scores.keys(), key=lambda doc: similarity_scores[doc], reverse=True)\n",
    "    return ranked_documents, similarity_scores\n",
    "\n",
    "output_filename = \"RRDV-consultas_resultados.tsv\"\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    print(\"Results in {} already exist\".format(output_filename))\n",
    "else:\n",
    "    # Write results to file\n",
    "    results_file = open(output_filename, \"w\")\n",
    "\n",
    "    # Iterate over queries\n",
    "    for query_file in os.listdir(queries_directory):\n",
    "        if query_file.endswith('.naf'):\n",
    "            query_path = os.path.join(queries_directory, query_file)\n",
    "            query_text = extract_raw_text(query_path)\n",
    "            \n",
    "            ranked_documents, similarity_scores = retrieve_and_rank_documents(query_text, inverted_index, term_freq_per_document)\n",
    "            \n",
    "            # Write results to the file\n",
    "            result_line = query_file[8:-4] + \"\\t\" + \",\".join([f\"{doc[8:]}:{similarity_scores[doc]:.6f}\" for doc in ranked_documents])\n",
    "            results_file.write(result_line + \"\\n\")\n",
    "            print(\"finished query {}\".format(query_file))\n",
    "\n",
    "    results_file.close()\n",
    "    print(\"Results written to \"+ output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Query:\n",
      "Query: q01\n",
      "P@M: 0.0000, R@M: 0.0000, NDCG@M: 0.9705\n",
      "\n",
      "Query: q02\n",
      "P@M: 0.5455, R@M: 0.5455, NDCG@M: 0.8576\n",
      "\n",
      "Query: q03\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 0.9717\n",
      "\n",
      "Query: q04\n",
      "P@M: 0.5714, R@M: 0.5714, NDCG@M: 0.9756\n",
      "\n",
      "Query: q06\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 0.8140\n",
      "\n",
      "Query: q07\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.9853\n",
      "\n",
      "Query: q08\n",
      "P@M: 0.6667, R@M: 0.6667, NDCG@M: 0.8914\n",
      "\n",
      "Query: q09\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 1.0000\n",
      "\n",
      "Query: q10\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.8358\n",
      "\n",
      "Query: q12\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.9891\n",
      "\n",
      "Query: q13\n",
      "P@M: 0.6000, R@M: 0.6000, NDCG@M: 0.8077\n",
      "\n",
      "Query: q14\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 0.8661\n",
      "\n",
      "Query: q16\n",
      "P@M: 0.0000, R@M: 0.0000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q17\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.9281\n",
      "\n",
      "Query: q18\n",
      "P@M: 0.5714, R@M: 0.5714, NDCG@M: 0.9205\n",
      "\n",
      "Query: q19\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q22\n",
      "P@M: 0.4286, R@M: 0.4286, NDCG@M: 1.0000\n",
      "\n",
      "Query: q23\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.8071\n",
      "\n",
      "Query: q24\n",
      "P@M: 0.4000, R@M: 0.4000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q25\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 1.0000\n",
      "\n",
      "Query: q26\n",
      "P@M: 0.0000, R@M: 0.0000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q27\n",
      "P@M: 0.3750, R@M: 0.3750, NDCG@M: 0.8556\n",
      "\n",
      "Query: q28\n",
      "P@M: 0.3333, R@M: 0.3333, NDCG@M: 1.0000\n",
      "\n",
      "Query: q29\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 0.9489\n",
      "\n",
      "Query: q32\n",
      "P@M: 0.8000, R@M: 0.8000, NDCG@M: 0.8961\n",
      "\n",
      "Query: q34\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q36\n",
      "P@M: 0.9000, R@M: 0.9000, NDCG@M: 0.9188\n",
      "\n",
      "Query: q37\n",
      "P@M: 0.6667, R@M: 0.6667, NDCG@M: 0.9602\n",
      "\n",
      "Query: q38\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 0.9307\n",
      "\n",
      "Query: q40\n",
      "P@M: 0.7778, R@M: 0.7778, NDCG@M: 0.8878\n",
      "\n",
      "Query: q41\n",
      "P@M: 0.8571, R@M: 0.8571, NDCG@M: 0.8814\n",
      "\n",
      "Query: q42\n",
      "P@M: 0.3333, R@M: 0.3333, NDCG@M: 0.8660\n",
      "\n",
      "Query: q44\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 0.8489\n",
      "\n",
      "Query: q45\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.9560\n",
      "\n",
      "Query: q46\n",
      "P@M: 0.1667, R@M: 0.1667, NDCG@M: 0.8875\n",
      "\n",
      "MAP: 0.5956\n"
     ]
    }
   ],
   "source": [
    "from metrics import precision_at_k, recall_at_k, ndcg_at_k, mean_average_precision\n",
    "\n",
    "# filtra los resultados de las listas para solo incluir los nombres de los documentos\n",
    "def filter_result_list(input_list):\n",
    "    return[full_result[:full_result.find(\":\")] for full_result in input_list]\n",
    "\n",
    "# Load the relevance judgments from the file\n",
    "relevance_judgments_file = open(relevance_judgments_directory, \"r\")\n",
    "relevance_judgments = {}\n",
    "\n",
    "for line in relevance_judgments_file:\n",
    "    query, judgments = line.strip().split('\\t')\n",
    "    relevance_judgments[query] = judgments.split(',')\n",
    "\n",
    "relevance_judgments_file.close()\n",
    "\n",
    "# Load the query results from the file\n",
    "query_results_file = open(\"RRDV-consultas_resultados.tsv\", \"r\")\n",
    "query_results = {}\n",
    "\n",
    "for line in query_results_file:\n",
    "    query, results = line.strip().split('\\t')\n",
    "    query_results[query] = results.split(',')\n",
    "\n",
    "query_results_file.close()\n",
    "\n",
    "# Calculate metrics for each query\n",
    "metrics_by_query = {}\n",
    "\n",
    "for query in query_results:\n",
    "    query_judgments = relevance_judgments.get(query, [])\n",
    "    query_results_list = query_results[query]\n",
    "    query_results_list_filtered = filter_result_list(query_results_list)\n",
    "    query_judgments_filtered = filter_result_list(query_judgments)\n",
    "    \n",
    "    if query in relevance_judgments:\n",
    "        # Convert relevance judgments to binary values\n",
    "        relevance_vector = [1 if doc in query_judgments_filtered else 0 for doc in query_results_list_filtered]\n",
    "        \n",
    "        # Calculate Precision@M and Recall@M\n",
    "        precision = precision_at_k(relevance_vector, len(query_judgments))\n",
    "        recall = recall_at_k(relevance_vector, len(query_judgments), len(query_judgments))\n",
    "        \n",
    "        # Calculate NDCG@M using non-binary relevance values\n",
    "        ndcg = ndcg_at_k([int(judgment[judgment.find(\":\")+1:]) for judgment in query_judgments], len(query_judgments))\n",
    "        \n",
    "        metrics_by_query[query] = {\"precision\": precision, \"recall\": recall, \"ndcg\": ndcg}\n",
    "    else:\n",
    "        metrics_by_query[query] = {\"precision\": 0.0, \"recall\": 0.0, \"ndcg\": 0.0}\n",
    "\n",
    "# Calculate MAP\n",
    "binary_relevance_vectors = []\n",
    "for query in metrics_by_query:\n",
    "    if query in relevance_judgments:\n",
    "        query_judgments = relevance_judgments[query]\n",
    "        query_judgments_filtered = filter_result_list(query_judgments)\n",
    "        \n",
    "        query_results_list = query_results[query]\n",
    "        query_results_list_filtered = filter_result_list(query_results_list)\n",
    "\n",
    "        # Convert relevance judgments to binary values\n",
    "        relevance_vector = [1 if doc in query_judgments_filtered else 0 for doc in query_results_list_filtered]\n",
    "        binary_relevance_vectors.append(relevance_vector)\n",
    "\n",
    "map_value = mean_average_precision(binary_relevance_vectors)\n",
    "\n",
    "print(\"Metrics by Query:\")\n",
    "for query, metrics in metrics_by_query.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"P@M: {metrics['precision']:.4f}, R@M: {metrics['recall']:.4f}, NDCG@M: {metrics['ndcg']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"MAP: {map_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
