{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "## Punto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de estrategias de motores de búsqueda\n",
    "A continuación, implementará un motor de búsqueda con cuatro estrategias diferentes.\n",
    "1. Búsqueda binaria usando índice invertido (BSII)\n",
    "3. Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "Debe hacer su propia implementación usando numpy y pandas.\n",
    "\n",
    "Conjunto de datos: hay tres archivos que componen el conjunto de datos: \n",
    "- “Docs raws texts\" contiene 331 documentos en formato NAF (XML; debe usar el título y el \n",
    "contenido para modelar cada documento). \n",
    "- \"Queries raw texts\" contiene 35 consultas. \n",
    "- \"relevance-judgments.tsv\" contiene para cada consulta los documentos considerados relevantes \n",
    "para cada una de las consultas. Estos documentos relevantes fueron construidos manualmente por \n",
    "jueces humanos y sirven como “ground-truth” y evaluación.\n",
    "\n",
    "Pasos de preprocesamiento: para los siguientes puntos, debe preprocesar documentos y consultas \n",
    "mediante tokenización a nivel de palabra, eliminación de palabras vacías, normalización y stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'docs-raw-texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m target_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_name)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Create the folder within the target directory\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m os\u001b[39m.\u001b[39mmkdir(target_folder)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Extract all files to the target folder\u001b[39;00m\n\u001b[0;32m     16\u001b[0m zip_ref\u001b[39m.\u001b[39mextractall(target_folder)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'docs-raw-texts'"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        # Create the folder within the target directory\n",
    "        os.mkdir(target_folder)\n",
    "        \n",
    "        # Extract all files to the target folder\n",
    "        zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Directory containing the extracted XML files\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "# NLTK setup\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the document text\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text.strip().lower())\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        \n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract content from the XML\n",
    "        content = root.find('raw').text\n",
    "        \n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "\n",
    "print(\"Inverted index created.\")\n",
    "\n",
    "# You can now save the inverted index to a variable or a file as needed\n",
    "# For example, to save it to a variable:\n",
    "inverted_index_variable = inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wes2015.d326.naf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index_variable[preprocess_text(\"Jahren\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
